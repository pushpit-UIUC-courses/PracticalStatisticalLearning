---
title: 'CS 598: Homework 5'
author: "Pushpit Saxena (netid: pushpit2)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(include = TRUE)  # TRUE for solution; FALSE for questions set

  knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_chunk$set(message = FALSE)
  knitr::opts_chunk$set(warning = FALSE)
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
  options(width = 90)
```

```{css, echo=FALSE}
.solution {
background-color: #e6ffe6;
}
```

```{r include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
if(!require(ElemStatLearn)) install.packages("ElemStatLearn", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(ramify)) install.packages("ramify", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(reshape)) install.packages("reshape", repos = "http://cran.us.r-project.org")
if (!require(Rfast)) install.packages("Rfast", repos = "http://cran.us.r-project.org")
```

## Question 1 [50 Points] K-Means Clustering

  * Load the `zip.train`
```{r}
library(ElemStatLearn)
data(zip.train)
X <- as.matrix(zip.train[, -c(1)])
Y <- as.matrix(zip.train[, 1])
C <- array(numeric(), dim=c(k,256))
set.seed(13)
for (i in 1:k){
  C[i,] <-runif(ncol(X), min = apply(X, 2, min), max = apply(X, 2, max))
}

```

* Generating Principle components of the data (to be used in the plots latter):
```{r}
pr1 <- prcomp(X, scale = T)
plot(pr1$x[,1], pr1$x[,2], xlab="PC 1",
     ylab="PC 2",
     main=" Scatter plot of the first two principle components (before clustering)")
```

 * Write your own k-means:
```{r}
library(Rfast)
clusterAssignment <- function(X,c) {
  distMat <- dista(X, c)
  clusters <- as.matrix(apply(distMat, 1, which.min))
  return(list("clusters" = clusters, "distMat" = distMat))
}

mykmeans <- function(X, C, K=5) {
  centroids <- C
  clustersAndDist <- clusterAssignment(X, centroids)
  iter <- 1
  while (TRUE) {
    oldCluster <- clustersAndDist
    newCentroids <-
      t(sapply(1:K, function(c) colMeans(X[which(clustersAndDist$clusters == c), ])))
    clustersAndDist <- clusterAssignment(X, newCentroids)
    if (identical(oldCluster$clusters, clustersAndDist$clusters)) {
      centroids <-
        t(sapply(1:K, function(c) colMeans(X[which(clustersAndDist$clusters == c), ])))
      # cat("Kmeans converged at iteration: ", iter, "\n")
      break
    }
    iter <- iter + 1
  }
  dataForWithinness <- 
    data.frame(cluster=clustersAndDist$clusters,
               distance = apply(clustersAndDist$distMat, 1, min) ^ 2)
  diff.d <- aggregate(dataForWithinness[, 2], list("cluster" = dataForWithinness$cluster), sum)
  mytotWithinss <- sum(diff.d$x)
  return (list("clusters" = clustersAndDist, "centroids" = centroids,
               "tot.withiness" = mytotWithinss, "covergence_iter" = iter))
}
```
 * Perform your algorithm with one random initialization with $k = 5$
```{r}
mykmean.fit <- mykmeans(X, C)
library(dplyr)
C <- array(numeric(), dim=c(5,10))
for (i in 1:nrow(Y)) {
  cluster_id <- mykmeans.fit$
}


```



print(mykmean.fit$tot.withiness)
plot.kmeans <- function(pr1, clusters) {
  df = data.frame("PC1" = pr1$x[,1], "PC2" = pr1$x[,2], "Cluster" = as.factor(mykmean.fit$clusters$clusters))
  centroids <- aggregate(df[, 1:2], list("cluster"=df$Cluster), mean)

  library(ggplot2)
  ggplot(df, aes(df$PC1,df$PC2, color=df$Cluster, shape=df$Cluster)) + geom_point() + stat_ellipse(type="norm", linetype=2) + stat_ellipse(type="t")
  # ggplot(df, aes(df$PC1,df$PC2)) + geom_point(aes(color=df$Cluster, shape=df$Cluster)) + geom_point(data = centroids, aes(x=centroids$PC1, y=centroids$PC2), size=5)
}

calc.mykmeans.totwithinnes <- function(mykmeans.fit) {
  dataForWithinness <- data.frame(cluster=mykmean.fit$clusters$clusters, distance = apply(mykmean.fit$clusters$distMat, 1, min) ^ 2)
# dataForWithinness

  diff.d <- aggregate(dataForWithinness[, 2], list("cluster" = dataForWithinness$cluster), sum)
  mytotWithinss <- sum(diff.d$x)
  return(mytotWithinss)
}

kmeans.fit <- kmeans(X, C)
kmeans.fit$tot.withinss - mytotWithinss

kmeans.fit$tot.withinss


```

```{r}
set.seed(1513)
C <- array(numeric(), dim=c(k,256))
bestMyKmeansFit <- NULL
best_C <- NULL
for (iter in 1:10) {
  for (i in 1:k){
    C[i,] <-runif(ncol(X), min = apply(X, 2, min), max = apply(X, 2, max))
  }
  # kmeans.fit <- kmeans(X, C)
  mykmeans.fit <- mykmeans(X, C)
  if (iter == 1 || mykmeans.fit$tot.withiness < bestMyKmeansFit$tot.withiness) {
    bestMyKmeansFit <- mykmeans.fit
    best_C <- C
  } 
}

bestMyKmeansFit$tot.withiness
kmeans_fit <- kmeans(X, best_C)

bestMyKmeansFit$tot.withiness - kmeans_fit$tot.withinss 

plot.kmeans(pr1, bestMyKmeansFit$clusters$clusters)

plot.kmeans(pr1, kmeans_fit$cluster)

length(kmeans_fit$cluster == bestMyKmeansFit$clusters$clusters)
```


  
  * [10 Points] Perform your algorithm with one random initialization with $k = 5$
    + For this question, compare your cluster membership to the true digits. What are the most prevalent digits in each of your clusters?
  * [10 Points] Perform your algorithm with 10 independent initiations with $k = 5$ and record the best
    + For this question, plot your clustering results on a two-dimensional plot, where the two axis are the first two principle components of your data
  * [15 Points] Compare the clustering results from the above two questions with the built-in `kmeans()` function in R. Use tables/figures to demonstrate your results and comment on your findings.

## Question 1 [50 Points] Two-dimensional Gaussian Mixture Model

We consider an example of the EM algorithm, which fits a Gaussian mixture model to the Old Faithful eruption data. For a demonstration of this problem, see the figure provided on [Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm). As the end result, we will obtain the distribution parameters of the two underlying distributions. We consider the problem as follows. For this question, you are allowed to use packages that calculate the densities of normal distributions. 

* We use both variables `eruptions` and `waiting`. We assume that the underlying distributions given the unobserved latent variables are both two-dimensional normal: $N(\mu_1, \Sigma_1)$ and $N(\mu_2, \Sigma_2)$, respectively, while $\mu_1$, $\Sigma_1$, $\mu_2$, and $\Sigma_2$ are unknow parameters that we need to solve.
* We assume that the unobserved latent variables (that indicate the membership) follow i.i.d. Bernoulli distribution, with parameter $p$.
* Based on the logic of an EM algorithm, we will first initiate some values of the parameters in the normal distribution. I provided a choice of them, and the normal density plots based on the initial values.
* Your goal is to write the EM algorithm that progressively updates the parameters and the latent variable distribution parameter. Eventually, we will reach a stable model fitting result that approximate the two underlying distributions, as demonstrated on the Wikipedia page. Choose a reasonable stopping criterion. To demonstrate your results, you should provide at least the following information. 
  + The distribution parameters $\mu_1$, $\Sigma_1$, $\mu_2$, and $\Sigma_2$
  + A histogram of the underlying probabilities of the latent variables
  + Plot the normal densities at the 2nd, 3rd, 4th and the final iteration of your algorithm
* Now, experiment a very different initial value of the parameters and rerun the algorithm. Comment on the efficiency and convergence speed of this algorithm.  

```{r}
  # load the data
  faithful = as.matrix(read.table("faithful.txt"))

  # the parameters
  mu1 = c(3, 80)
  mu2 = c(3.5, 60)
  Sigma1 = matrix(c(0.1, 0, 0, 10), 2, 2)
  Sigma2 = matrix(c(0.1, 0, 0, 50), 2, 2)
  library(mixtools)
  plot(faithful)
  
  addellipse <- function(mu, Sigma, ...)
  {
    ellipse(mu, Sigma, alpha = .05, lwd = 2, ...)
    ellipse(mu, Sigma, alpha = .25, lwd = 3, ...)
    ellipse(mu, Sigma, alpha = .01, lwd = 1, ...)
  }
    
  addellipse(mu1, Sigma1, col = "darkorange")
  addellipse(mu2, Sigma2, col = "deepskyblue")
```
```{r}
X <- faithful
mu = rbind(mu1, mu2)
mu_mem <- mu
w <- c(0.5, 0.5)
mvpdf <- function(x, mu, sigma) {
    if (det(sigma) == 0) {
        warning("Determinant is equal to 0.")
    }
    apply(x, 1, function(x) exp(-(1/2) * (t(x) - mu) %*% MASS::ginv(sigma) %*% 
        t(t(x) - mu))/sqrt(det(2 * pi * sigma)))
}
km.fit <- kmeans(X, 2)

gmm.fromscratch <- function(X, k, plot = F){
  p <- ncol(X)  # number of parameters
  n <- nrow(X)  # number of observations
  Delta <- 1; iter <- 0; itermax <- 100

  if(iter == 0){
    mu <- rbind(mu1, mu2)
    mu_mem <- mu
    a <- c(0.8, 0.2)
    # w <- w/sum(w)
    cov <- list(Sigma1, Sigma2)
    # for(i in 1:p) for(j in 1:p) for(c in 1:k) cov[i, j, c] <- 
    #   1/n * sum((X[km.init$cluster == c, i] - mu[c, i]) *
    #   (X[km.init$cluster == c, j] - mu[c, j]))
  }

  llh <- 0
  iter <- 1;
  pi <- 0.5
  log_pi <- 0
  while(Delta >= 1e-3 && iter <=30 ){
      cat("Running iteration: ", iter, "\n")
    # E-step
    # mvn.c <- dmvnorm(X, mu1, Sigma1)
    z <- cbind(dmvnorm(X, mu[1, ], sigma = cov[[1]]), dmvnorm(X, mu = mu[2, ], sigma = cov[[2]]))
    print(dim(z))

    r <- cbind((pi * z[, 1])/rowSums(t((t(z) * pi))), (1-pi * z[, 2])/rowSums(t((t(z) *
    (1-pi)))))
    
    # print(dim(r))
    # print(r[,1])
    new_llh <- sum(log(((1-pi) * z[,1]) + (pi * z[,2])))
    gamma <- (pi * z[,2])/(((1-pi) * z[,1]) + (pi *z[,2]))
    # print(gamma)
    pi <- mean(gamma)
    # Total Responsibility
    # mc <- colSums(r)
    # print(length(mc))
    # a <- mc[2]/ nrow(X)
    
    # Update our Means
    mu <- rbind(colSums((1-gamma) * X)/sum(1-gamma), colSums((gamma * X)/sum(gamma)))
    # print(mu)
    # print(dim(t(X-mu[1,]) %*% (X-mu[1,])))
    # # print(X[1,])
    
    # 
    # muls <- sapply(1:nrow(X), function(i) {
    #   ddd <- (X[i,] - mu[1,]) %*% t(X[i, ] - mu[1, ])
    #   mul <- (1 - gamma[i]) * ddd
    # }, simplify = FALSE)
    # numer <- Reduce("+", muls)
    cov[[1]] <- Reduce("+", sapply(1:nrow(X), function(i) {
      ddd <- (X[i,] - mu[1,]) %*% t(X[i, ] - mu[1, ])
      mul <- (1 - gamma[i]) * ddd
    }, simplify = FALSE))/sum(1-gamma)
    # print(cov[[1]])
    # mu <- rbind(colSums(X * r[, 1]) * 1/mc[1], colSums(X * r[, 2]) * 1/mc[2])
    # cov[[1]] <- t(r[, 1] * t(apply(X, 1, function(x) x - mu[1, ]))) %*% 
    # (r[, 1] * t(apply(X, 1, function(x) x - mu[1, ]))) * 1/mc[1]
    
    cov[[2]] <- Reduce("+", sapply(1:nrow(X), function(i) {
      ddd <- (X[i,] - mu[2,]) %*% t(X[i, ] - mu[2, ])
      mul <- gamma[i] * ddd
    }, simplify = FALSE))/sum(gamma)
    
    # Delta <- sum((mu - mu_mem) ^ 2)
    Delta <- abs(llh - new_llh)
    llh <- new_llh
    cat("Delta: ", Delta, " New llh: ", llh, "\n")
    # new_llh <- sum(log(mc))
    # Delta <- abs(llh - new_llh)
    # llh <- new_llh
    
    plot(faithful, xlim = c(1, 6), ylim = c(35, 100), main=paste("Iteration: ", iter, sep=" "))
    addellipse(mu[1,], cov[[1]], col = "darkorange")
    addellipse(mu[2,], cov[[2]], col = "deepskyblue")
    mu_mem <- mu; iter <- iter+1
    
  }
  
  cat("Final iteration: ", iter, " Done \n")
  return(list(means=mu, cov=cov))
}
#     r_ic <- t(w*t(mvn.c)) / rowSums(t(w*t(mvn.c)))
# # M-step
#     n_c <- colSums(r_ic)
#     w <- n_c/n
#     mu <- t(sapply(1:k, function(c) 1/n_c[c] * colSums(r_ic[, c] * X)))
#     
#     for(i in 1:p) for(j in 1:p) for(c in 1:k) cov[i, j, c] <- 
#       1/n_c[c] * sum(r_ic[, c] * (X[, i] - mu[c, i]) * r_ic[, c] *
#       (X[, j] - mu[c, j]))
#     cluster <- apply(r_ic, 1, which.max)
#     Delta <- sum((mu - mu_mem)^2)
#     iter <- iter + 1; mu_mem <- mu
#   }
#   
#   cat("Final iteration: ", iter, " Done \n")
#   return(list(softcluster = r_ic, cluster = cluster, means=mu, cov=cov))
# }
gmm <- gmm.fromscratch(X, 2, plot = F)

# 
# cov
plot(faithful, xlim = c(1, 6), ylim = c(35, 100))
addellipse(gmm$means[1,], gmm$cov[[1]], col = "darkorange")
addellipse(gmm$means[2,], gmm$cov[[2]], col = "deepskyblue")
# table(y, gmm$cluster)
# library(magick)
# list.files(path = paste0(wd, "/figs_gmm/"), pattern = "*.png", full.names = T) %>%
#   image_read() %>%
#   image_join() %>%
#   image_animate(fps=1) %>%
  # image_write("fig_gmm_anim.gif")
```











