{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T04:57:48.787896Z",
     "start_time": "2019-10-27T04:57:48.783435Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm, multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:08:32.502598Z",
     "start_time": "2019-10-27T06:08:32.495104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"faithful.txt\", sep=\"\\s+\")\n",
    "X = df.to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    \"\"\"\n",
    "    Full covariance Gaussian Mixture Model,\n",
    "    trained using Expectation Maximization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components, n_iter, tol, seed):\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X):\n",
    "        \n",
    "        # data's dimensionality and responsibility vector\n",
    "        self.n_row, self.n_col = X.shape     \n",
    "        self.resp = np.zeros((self.n_row, self.n_components))\n",
    "        \n",
    "        means = np.array([[3, 80], [3.5, 60]])\n",
    "        Sigma1 = np.array([[0.1, 0.0], [0.0, 10.0]])\n",
    "        Sigma2 = np.array([[0.1, 0], [0, 50]])\n",
    "        shape = 2, 2, 2\n",
    "        covs = np.full(shape, np.cov(X, rowvar = False))#np.empty(shape, dtype=float)\n",
    "        covs[0, :, :] = Sigma1\n",
    "        covs[1,:, :] = Sigma2\n",
    "        # initialize parameters\n",
    "        np.random.seed(self.seed)\n",
    "        chosen = np.random.choice(self.n_row, self.n_components, replace = False)\n",
    "        self.means = np.array([[3, 80], [3.5, 60]]) # X[chosen]\n",
    "        self.weights = np.full(self.n_components, 1 / self.n_components)\n",
    "        \n",
    "        # for np.cov, rowvar = False, \n",
    "        # indicates that the rows represents obervation\n",
    "        shape = self.n_components, self.n_col, self.n_col\n",
    "        self.covs = covs#np.full(shape, np.cov(X, rowvar = False))\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        self.converged = False\n",
    "        self.log_likelihood_trace = []      \n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            self._do_estep(X)\n",
    "            self._do_mstep(X)\n",
    "            log_likelihood_new = self._compute_log_likelihood(X)\n",
    "           \n",
    "            if (log_likelihood - log_likelihood_new) <= self.tol:\n",
    "                self.converged = True\n",
    "                break\n",
    "                \n",
    "            log_likelihood = log_likelihood_new\n",
    "            self.log_likelihood_trace.append(log_likelihood)\n",
    "            \n",
    "        return self\n",
    "      \n",
    "    def _do_estep(self, X):\n",
    "        \"\"\"\n",
    "        E-step: compute responsibilities,\n",
    "        update resp matrix so that resp[j, k] is the responsibility of cluster k for data point j,\n",
    "        to compute likelihood of seeing data point j given cluster k, use multivariate_normal.pdf\n",
    "        \"\"\"\n",
    "        for k in range(self.n_components):\n",
    "            prior = self.weights[k]\n",
    "            likelihood = multivariate_normal(self.means[k], self.covs[k]).pdf(X)\n",
    "            self.resp[:, k] = prior * likelihood\n",
    "        \n",
    "        # normalize over all possible cluster assignments\n",
    "        self.resp = self.resp / self.resp.sum(axis = 1, keepdims = 1)\n",
    "        return self\n",
    " \n",
    "    def _do_mstep(self, X):\n",
    "        \"\"\"M-step, update parameters\"\"\"\n",
    "        \n",
    "        # total responsibility assigned to each cluster, N^{soft}\n",
    "        resp_weights = self.resp.sum(axis = 0)\n",
    "        \n",
    "        # weights\n",
    "        self.weights = resp_weights / self.n_row\n",
    "        \n",
    "        # means\n",
    "        weighted_sum = np.dot(self.resp.T, X)\n",
    "        self.means = weighted_sum / resp_weights.reshape(-1, 1)\n",
    "        \n",
    "        # covariance\n",
    "        for k in range(self.n_components):\n",
    "            diff = (X - self.means[k]).T\n",
    "            weighted_sum = np.dot(self.resp[:, k] * diff, diff.T)\n",
    "            self.covs[k] = weighted_sum / resp_weights[k]\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"manually compute the log likelihood of the current parameter\"\"\"\n",
    "        log_likelihood = 0\n",
    "        for k in range(self.n_components):\n",
    "            \n",
    "            weight = self.weights[k]\n",
    "            mean = self.means[k]\n",
    "            cov = self.covs[k]\n",
    "            cov_inverse = np.linalg.inv(cov)\n",
    "            term_other = np.log(2 * np.pi) + np.log(np.linalg.det(cov))\n",
    "            \n",
    "            for x in X:\n",
    "                # compute (x-mu)^T * Sigma^{-1} * (x-mu)\n",
    "                diff = x - mean\n",
    "                term_exponent = np.dot(diff.T, np.dot(cov_inverse, diff))\n",
    "            \n",
    "                # compute loglikelihood contribution for this data point and this cluster             \n",
    "                log_likelihood += -1 / 2 * (term_other + term_exponent) + np.log(weight)\n",
    "            \n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T06:07:00.305337Z",
     "start_time": "2019-10-27T06:07:00.296163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[[ 0.1  0. ]\n",
      "  [ 0.  10. ]]\n",
      "\n",
      " [[ 0.1  0. ]\n",
      "  [ 0.  50. ]]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "def plot_contours(data, means, covs, title):\n",
    "    \"\"\"visualize the gaussian components over the data\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(data[:, 0], data[:, 1], 'ko')\n",
    "\n",
    "    delta = 0.025\n",
    "    k = means.shape[0]\n",
    "    x = np.arange(-2.0, 7.0, delta)\n",
    "    y = np.arange(-2.0, 7.0, delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    col = ['green', 'red', 'indigo']\n",
    "    for i in range(k):\n",
    "        mean = means[i]\n",
    "        cov = covs[i]\n",
    "        sigmax = np.sqrt(cov[0][0])\n",
    "        sigmay = np.sqrt(cov[1][1])\n",
    "        sigmaxy = cov[0][1] / (sigmax * sigmay)\n",
    "        Z = mlab.bivariate_normal(X, Y, sigmax, sigmay, mean[0], mean[1], sigmaxy)\n",
    "        plt.contour(X, Y, Z, colors = col[i])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "np.random.seed(10)\n",
    "chosen = np.random.choice(X.shape[0], 2, replace = False)\n",
    "means = X[chosen]\n",
    "print(means.shape)\n",
    "# means = np.array([[3, 80], [3.5, 60]])\n",
    "Sigma1 = np.array([[0.1, 0.0], [0.0, 10.0]])\n",
    "Sigma2 = np.array([[0.1, 0], [0, 50]])\n",
    "shape = 2, 2, 2\n",
    "covs = np.full(shape, np.cov(X, rowvar = False))#np.empty(shape, dtype=float)\n",
    "covs[0, :, :] = Sigma1\n",
    "covs[1,:, :] = Sigma2\n",
    "print(covs)\n",
    "# plot_contours(X, means, covs, \"Initial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
