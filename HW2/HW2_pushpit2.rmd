---
title: 'STAT 542 / CS 598: Homework 2'
author: "Pushpit Saxena (netId: pushpit2)"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
---
#### Loading the necessary libraries
```{r results="hide", warning=FALSE, message=FALSE, echo=FALSE}
  library(mlbench)
  library(glmnet)
  library(zoo)
  library(magrittr)
  library(dplyr)
  library(caret)
```

# Question 1: Linear Model Selection

## Prepare the Boston Housing Data
```{r fig.width=5, fig.height=5}

  data(BostonHousing2)
  BH = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract"))]
  full.model <- lm(cmedv~., data = BH)
  summary(full.model)
```

Dimension of boston housing data

```{r}
  dim(BH)
```

## Question 1.a:  Report the most significant variable from this full model with all features.

### Answer: 

To answer this question based on the full model fitted on unscaled data, I have taken a look at the P-value of each of the predictors and picked the predictor with the least *P-value*: **lstat**

```{r}
  sort(summary(full.model)$coefficients[,4])[1]
```

## Question 1.b: Starting from this full model, use stepwise regression with both forward and backward and BIC criterion to select the best model. Which variables are removed from the full model?

### Answer: 
This can be done using the step function. I have taken the sample size of 506 as n and passed log(n) as k parameter to step function. Also setup the direction as "both" to have forward and backward. The variables that are removed are: **lon, lat, indus, age**

```{r}
n = dim(BH)[1]
stepBIC = step(full.model, direction = "both", k=log(n), trace = 0)
```

Model selected based on stepwise regression starting at full model (direction=both, criterion=BIC):
```{r}
summary(stepBIC)
```

Comparison with the full model:
```{r}
anova(full.model, stepBIC)
```


## Question 1.c: Starting from this full model, use the best subset selection and list the best model of each model size.

### Answer: 

I have used regsubsets function from leaps library to find the best model of each model size. I have used the which attribute of the summary of regsubsets object to list the best model of each model size. The matrix shown in the output shows the best model for each model size (Please note that True indicate the predictor is included in the model)

```{r tidy=TRUE}
p = 15
library(leaps)
b = regsubsets(cmedv~., data=BH, nvmax=p, nbest=1)
rs = summary(b, matrix = T)
# Listing the best model of each model size.
rs$which
```

Coefficients vector for each of these models can be obtained as (the following code can be uncommented to see the coefficient information, I have commented it out just to keep the output in the pdf file manageable):
```{r}
#coef(rs$obj, 1:15)
```

## Question 1.d: Use the Cp criterion to select the best model from part c). Which variables are removed from the full model? What is the most significant variable?

### Answer:

As apparant from the plot below the best model as per the CP criterion is model with 11 predictors. Based on the models from part c; in the best model with 11 predcitors, the predictors that are dropped are: **lon, lat, indus, age**. The most significant variable (based on **P-value** similar to part a) is **lstat**

```{r fig.width=5, fig.height=5}
#msize = apply(rs$which, 1, sum)
msize = 1:15
#par(mfrow=c(1,2))
CP = rs$rss/(summary(full.model)$sigma^2) + 2 * msize - n
AIC = n*log(rs$rss/n) + 2*msize;
BIC = n*log(rs$rss/n) + msize*log(n);
#cbind(CP, rs$cp)
plot(msize, CP, xlab="No. of parameters", ylab="Cp", main="ModelSize(NumofParameters) vs Cp values")
cat("Number of parameters in the best model as per Cp criterion is ", which.min(CP))
```

The most significant predictor for best model with 11 predictors (based on **P-value** similar to part a) is : **lstat**
```{r}
lm_11_pred_model <- lm(cmedv ~ . -lon -lat -age -indus, data=BH)
sort(summary(lm_11_pred_model)$coefficients[,4])[1]
```

# Question 2: Code your own Lasso

## Data Preparation
```{r}
 library(MASS)
  set.seed(1)
  n = 200
  p = 200
  
  # generate data
  V = matrix(0.2, p, p)
  diag(V) = 1
  X = as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
  y = X[, 1] + 0.5*X[, 2] + 0.25*X[, 3] + rnorm(n)
  
  # we will use a scaled version 
  X = scale(X)
  Y = scale(y)
```

## Question 2.a: Hence, we need first to write a function that updates just one parameter, which is also known as the soft-thresholding function. Construct the function in the form of soft_th <- function(b, lambda), where b is a number that represents the one-dimensional linear regression solution, and lambda is the penalty level. The function should output a scaler, which is the minimizer of
$$ \big( \mathbf X - b)^\text{2} + \lambda |b| $$

### Answer:

Followed the mathematical derivation for soft-thresholding operator shown in the lectures slides and implemented it as below:

```{r}

soft_th <- function(b, lambda) {
  if (b > lambda/2) {
    return (b - (lambda / 2))
  } else if (abs(b) <= lambda/2) {
    return (0)
  } else if (b < -lambda/2) {
    return (b + (lambda/2))
  }
}
```

## Question 2.b: Single iteration

### Answer:

I have implemented a single loop of coordinate descent algorithm updating all the parameters one by one. The function is named **coord_descent** . This function will be used later in full implementation of mylasso function.

```{r}
coord_descent <- function(X, Y, b, r, lambda) {
  for(j in 1:p) {
    
       # calculating partial residuals
       r <- r + X[,j]*b[j]
  
       # updating beta and soft-thresholding
       xr <- sum(X[,j]*r)
       xx <- sum(X[,j]^2)
       b[j] <- xr/xx
       b[j] <- soft_th(b[j], lambda = lambda)
  
       # Re calculting residual (Gauss-Seidel style coordinate descent)
       # r <- Y - X%*%b
       r <- r - X[,j]*b[j]
       # print(b) 
  }
  return(list("b" = b, "r" = r))
}
lambda = 0.7
b = rep(0, p)
r = Y - X%*%b
obj = coord_descent(X, Y, b, r, lambda)
cat("First 3 observations in r after single loop:\n",
    paste(obj$r[1:3], collapse="\n "))
cat("\nNonzero entries in the updated beta_new vector:\n",
    paste(obj$b[which(obj$b != 0)], collapse="\n "))

```

## Question 2.c: My own implementation of lasso

### Answer:

```{r}

  mylasso <- function(X, Y, lambda, tol, maxitr) {
    p = dim(X)[2]
    b <- rep(0, p)
    r <- Y - X%*%b
    final_itr = maxitr
    for (itr in 1:maxitr) {
      b_old = b
      obj = coord_descent(X, Y, b, r, lambda)
      b = obj$b
      r = obj$r
      l1norm <- dist(rbind(b, b_old), method="manhattan")
      if (l1norm < tol) {
        #print(sprintf("Final iteration: %d, Final l1 distance: %f", itr, l1norm))
        final_itr = itr
        break
      }
      #print(sprintf("Iteration: %d, tol: %f, l1 distance: %f", itr, tol, l1norm))
    }
    
    # print(r[1:3])
    # print(b[which(b != 0)])
    return (list("final_itr" = final_itr, "b" = b, "r" = r))
  }
```

Running the method with lambda = 0.3, tol = 1e-5 and maxItr = 100
```{r}
lassoObj <- mylasso(X, Y, 0.3, 1e-5, 100)
```

i) The number of iterations took:
```{r}
lassoObj$final_itr
```

ii) The nonzero entries in the final beta parameter estimate:
```{r}
lassoObj$b[which(lassoObj$b != 0)]
```

iii) The first three observations of the residual:
```{r}
lassoObj$r[1:3]
```

## Question 2.d: Comparison with glmnet

### Answer: 

I have used the glmnet to do the lasso regression (setting alpha = 1) with lambda = 0.3 / 2 . The accuracy of our own implementation is almost similar to the one we get from glmnet. I have calculated the MSE for mylasso and glmnet model (shown below). Also, the beta-vectors differ by less than 0.005.

```{r}
 
  # setting the glmnet lambda to be at half i.e. 0.15

  mse_mylasso <- mean((Y - X%*%lassoObj$b)^2)
  glmnetlasso <- glmnet(X, Y, alpha=1, lambda = 0.15)
  mse_gl <- mean((Y -glmnetlasso %>% predict(X) %>% as.vector())^2)
  l1norm2 <- dist(rbind(lassoObj$b, glmnetlasso$beta[,1]), method="manhattan")
  cat("MSE (mylasso): ", mse_mylasso, "MSE(glmnet): ", mse_gl, "
      Diff between MSE: ", abs(mse_mylasso - mse_gl))
```

```{r}
  if (l1norm2 < 0.005) {
    cat("The distance between beta vector generated by mylasso 
        and glmnet (less the 0.005) is: ", l1norm2)
  }
```

# Question 3: Cross-Validation for Model Selection

## Question 3.a: Reading the data

### Answer:

Loading the necessary libraries

```{r results="hide"}
library(zoo)
library(magrittr)
library(dplyr)
library(caret)
options(na.action="na.omit")
```

#### Read data into R

```{r}
WalmartSales <- read.csv("Train.csv",header=TRUE,sep=",")
```

#### Convert character variables into factors 

```{r}
char <- c("Item_Fat_Content", "Item_Type", "Outlet_Identifier", 
            "Outlet_Size", "Outlet_Location_Type", "Outlet_Type")
WalmartSales[char] = lapply(WalmartSales[char], factor)
```

#### Remove Item_identifier and convert all factors into dummy variables
(I have also removed Outlet_Identifier as this predictor just like item_identifier was not contributing much to the model and after removing it the model performance increased slightly. I have tried removing rows/columns with NA values, but the best perfomance I got when I just impute the missing values in these numeric columns to mean values)
```{r}
ok <- sapply(WalmartSales, is.numeric)
WalmartSales[ok] <- lapply(WalmartSales[ok], na.aggregate)
# convert factors into dummies
WalMartData <- model.matrix( ~ . -1, data = WalmartSales[, -c(1,7)])
dim(WalMartData)
```

## Question 3.b: Cross-validation and model building

### Answer:

Randomly splitting the data. (Please note that the seed is set to make the results reproducible). The data is splitted into two equal parts (split ratio = 0.5) and then *Item_Outlet_Sales* column is taken out as Y values and converted to log scale as per the instructions.
```{r}
set.seed(1)
smp_size = floor(0.5 * nrow(WalMartData))
train_ind = sample(seq_len(nrow(WalMartData)),size = smp_size)
train_df = WalMartData[train_ind,]
test_df = WalMartData[-train_ind,]

X_train = train_df[, -dim(train_df)[2]]
Y_train = train_df[, dim(train_df)[2]]
Y_train = log(Y_train)

X_test = test_df[, -dim(test_df)[2]]
Y_test = test_df[, dim(test_df)[2]]
Y_test = log(Y_test)
```

### Cross-validation to select the best Lasso model

I have used the glmnet function to run the cross-fold validation and find the values for lambda.min and lambda.1se. Please find below the model fit information:
```{r}
set.seed(123)
cv_lasso <- cv.glmnet(X_train, Y_train, alpha = 1, nfold=50)
cat("For Lasso CV: Lambda.min:", cv_lasso$lambda.min, "Lambda.1se:", cv_lasso$lambda.1se)
```



#### Plot to show the MSE vs Log(Lambda) in cross-fold validation
```{r}
#Plot of log(lamda) vs MSE for lasso cross fold validation:
plot(cv_lasso)
```

#### Coefficient for lambda=lambda.min model:
```{r}
coef(cv_lasso, lambda=cv_lasso$lambda.min)
```

#### Coefficient for lambda = lamda.1se model:
```{r}
coef(cv_lasso, lambda=cv_lasso$lambda.1se)
```

#### Best Lasso model:

Based on the prediction done on test dataset, lasso with lambda = lambda.min is the best model (Please see below for results)
```{r}
y_pred_lasso_min <- predict(cv_lasso, newx=X_test, s=cv_lasso$lambda.min)
y_pred_lasso_1se <- predict(cv_lasso, newx=X_test, s=cv_lasso$lambda.1se)

mse_min <- mean((Y_test - y_pred_lasso_min)^2)
mse_1se <- mean((Y_test - y_pred_lasso_1se) ^2)
err_lasso_min <- postResample(y_pred_lasso_min, Y_test)
err_lasso_1se <- postResample(y_pred_lasso_1se, Y_test)

cat("Accuracy metrics for Lasso with Lamda.min: \nMSE: ", mse_min, "\n")
err_lasso_min

cat("Accuracy metrics for Lasso with Lamda.1se: \nMSE: ", mse_1se, "\n")
err_lasso_1se
```


### Cross-validation to select the best Ridge model

I have used the glmnet function to run the cross-fold validation and find the values for lambda.min and lambda.1se. Please find below the model fit information:
```{r}
set.seed(123)
cv_ridge <- cv.glmnet(X_train, Y_train, alpha = 0, nfold=50)
cat("For Ridge CV: Lambda.min:", cv_ridge$lambda.min, "Lambda.1se:", cv_ridge$lambda.1se)
```

#### Plot to show the MSE vs Log(Lambda) in cross-fold validation
```{r}
#Plot of log(lamda) vs MSE for ridge cross fold validation:
plot(cv_ridge)
```

#### Coefficient for lambda=lambda.min model:
```{r}
coef(cv_ridge, lambda=cv_ridge$lambda.min)
```

#### Coefficient for lambda=lambda.1se model:
```{r}
coef(cv_ridge, lambda=cv_ridge$lambda.1se)
```

#### Best Ridge model:
Based on the prediction done on test dataset, ridge with lambda = lambda.min is the best model (Please see below for results)

```{r}

y_pred_ridge_min <- predict(cv_ridge, newx=X_test, s=cv_ridge$lambda.min)
y_pred_ridge_1se <- predict(cv_ridge, newx=X_test, s=cv_ridge$lambda.1se)

mse_min <- mean((Y_test - y_pred_ridge_min)^2)
mse_1se <- mean((Y_test - y_pred_ridge_1se) ^2)
err_ridge_min <- postResample(y_pred_ridge_min, Y_test)
err_ridge_1se <- postResample(y_pred_ridge_1se, Y_test)

cat("Accuracy metrics for ridge with Lamda.min: \n", "MSE: ", mse_min, "\n")
err_ridge_min

cat("Accuracy metrics for Ridge with Lamda.1se: \n", "MSE:", mse_1se, "\n")
err_ridge_1se
```

### Test these four models on the testing data and report and compare the prediction accuracy:

Below are the comparison between all the four models. The best performing model is **Lasso regression with lambda=lambda.min**
```{r}
y_pred_lasso_min <- predict(cv_lasso, newx=X_test, s=cv_lasso$lambda.min)
y_pred_lasso_1se <- predict(cv_lasso, newx=X_test, s=cv_lasso$lambda.1se)
y_pred_ridge_min <- predict(cv_ridge, newx=X_test, s=cv_ridge$lambda.min)
y_pred_ridge_1se <- predict(cv_ridge, newx=X_test, s=cv_ridge$lambda.1se)

mse_min_ridge <- mean((Y_test - y_pred_ridge_min)^2)
mse_1se_ridge <- mean((Y_test - y_pred_ridge_1se) ^2)
mse_min_lasso <- mean((Y_test - y_pred_lasso_min)^2)
mse_1se_lasso <- mean((Y_test - y_pred_lasso_1se) ^2)

cat("MSE: \n Lasso(lambda.min): ", mse_min_lasso, "\n Lasso(Lambda.1se): ", mse_1se_lasso, "\n Ridge(Lambda.min): ", mse_min_ridge, "\n Ridge(Lambda.1se): ", mse_1se_ridge)

cat("\nR^2: \n Lasso(lambda.min): ", err_lasso_min[2], "\n Lasso(Lambda.1se): ", err_lasso_1se[2], "\n Ridge(Lambda.min): ", err_ridge_min[2], "\n Ridge(Lambda.1se): ", err_ridge_1se[2], "\n")

```
